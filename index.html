<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Zhikang Zou (邹智康)</title>
	<meta content="Zhikang Zou, https://BigTeacher-777.github.io/" name="keywords" />
	<style media="screen" type="text/css">
  html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: Arial, Helvetica, sans-serif;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #043d98;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}

a.paper {
  font-weight: bold;
  font-size: 12pt;
}

b.paper {
  font-weight: bold;
  font-size: 12pt;
}

* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 2em auto 2em auto;
  width: 825px;
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 14px;
  background: #F4F6F6;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  /* font-family: Lato, Verdana, Helvetica, sans-serif; */
  /* font-size: 13px; */
  font-weight:bold;
}

ul { 
  /* list-style: circle; */
  list-style: disc;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Arial, Helvetica, sans-serif;
  /*font-size: 13px;*/
  /* font-weight: bold; */
  color: #FF0000;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
   margin-bottom: 1.2em; 
  margin-top: 3em;
  /* background: #F4F6F6; */
  background: #FFFFFF;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em .6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
}
div.paper2 {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
}

/* div.paper:hover { */
    /* background: #FFFDEE; */
    /* background-color: #242d36 ; */
/* } */

div.paper2:hover {
    background: #FFFDEE;
    /* background-color: #242d36 ; */
}
div.bio {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .7em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 135%;
}

div.res {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.4em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.65em .8em 0.15em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 130%;
}

div.award {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.4em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.65em .8em 0.15em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 130%;
}

div.paper div {
  padding-left: 230px;
}

img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 13px;
  margin: 1em 0;
  padding: 0;
}

    .bot {
  font-size: 14%;
}

   .ptypej {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #5cb85c;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
   .ptypec {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #428bca;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
   .ptypep {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #6B6B6B;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
/* navigation */
#nav {
  /* font-family: 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans',*/
       /* Corbel, Arial, Helvetica, sans-serif; */
  /* font-family: Georgia, Helvetica, sans-serif; */
  /* position: fixed; */
  /* top: 50px; */
  /* left: 860px; */
  /* margin-left: 810px;     1060 */
  /* width: 92px; */
  /* font-size: 14px; */
  list-style-type: none;
    margin: 0;
    padding: 0;
    overflow: hidden;
    background-color: rgb(20, 68, 106);
    position: fixed;
    top: 0;
    width: 100%;
    left: 0;
    height: 3.8%;
    font-size: 16px;
    /* float: right */
}

#nav li {
    /* margin-bottom: 1px; */
    float: right;
}
ol {
  list-style: none;
}
#nav a {
    /* display: block;
    padding: 6px 9px 7px;
    color: #fff;
    background-color: #455A64;
    text-decoration: none; */
    display: block;
    color: white;
    text-align: center;
    padding: 11px 25px;
    text-decoration: none;
}

#nav a:hover {
    color: #ffde00;
    /* background-color: #242d36 ; */
}
</style>

<script type="text/javascript" src="./files/hidebib.js"></script>
</head>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
</script>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>

<!-- <script src="./files/main.js"></script> -->

<body >
  
  <ol id="nav">
    <li><a href="zhikangzou001@gmail.com" title="Contact">Contact</a></li>
    <li><a href="#pub" title="Papers">Papers</a></li>
    <li><a href="#news" title="News">News</a></li>
    <li><a href="#home" title="Home">Home</a></li>
  </ol>

<!-- Home section -->
<a name="home"><br /><br /><br /></a>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Zhikang Zou (邹智康)" style="float: left; padding-left: .01em; height: 140px;" src="zouzhikang.jpeg" alt="Zhikang Zou" />
<div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 180%; font-size: 20pt;">Zhikang Zou (邹智康)</span><br />
<span style="font-size: 12pt">Baidu VIS R&D Engineer</span><br />
<span style="font-size: 12pt">Department of Computer Vision Technology (VIS) </span> <br />
<span style="font-size: 12pt">Baidu Inc.</span><br /><br /></div>
<a href='https://scholar.google.com/citations?user=T-YePFgAAAAJ&hl=zh-CN' style="padding-left: 3.7em; font-size: 12pt">Google Scholar</a>&nbsp;/&nbsp;<a href='https://github.com/BigTeacher-777' style="font-size: 12pt">Github</a>&nbsp;/&nbsp;<a href='mailto:zhikangzou001@gmail.com' style="font-size: 12pt">Email</a>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section" style="padding-left: 0.8em; padding-top: 1em">
<!-- <h2>(<a href='https://scholar.google.com/citations?user=kWADCMUAAAAJ&hl=zh-CN'>Google scholar</a>)</h2> -->
<h2>About me</h2>
<div class="bio" style="font-size: 12pt;">

I am currently working at Baidu Inc. as a computer vision researcher, working with <a href='https://shuluoshu.github.io/'>Xiaoqing Ye</a> and <a href='https://scholar.google.com/citations?user=R1rVRUkAAAAJ&hl=zh-CN'>Xiao Tan</a>. I received my bachelor degree and master degree from Huazhong University of Science and Technology in 2020, advised by <a href='https://scholar.google.com/citations?user=cTpFPJgAAAAJ&hl=en'>Prof. Pan Zhou</a>. I had a great time as a research intern at Tecent AI lab at Shenzhen, advised by <a href='https://linchaobao.github.io/'>Linchao Bao</a>. My research interests are in computer vision and machine learning, with specific interest in 3D vision and crowd counting.
  
</div>
</div>

<a name="news"></a>
<div style="clear: both;">
<div class="section" style="padding-left: 0.8em; padding-top: 1em">
  <h2>News</h2>
  <div class="paper" style="font-size: 12pt">
    <ul>
    <li> 2022.06: Two paper accepted by <font color="DarkRed">ACMMM2022</font>.</li>
    <li> 2022.06: One paper accepted by <font color="DarkRed">RA-L2022</font>.</li>
    <li> 2021.07: Two paper accepted by <font color="DarkRed">ICCV2021</font>. It is also my first monocular 3D detection paper.</li>
    <li> 2021.07: Three paper accepted by <font color="DarkRed">ACMMM2021</font>.</li>
    <li> 2021.05: We <font color="Red">rank second</font> in the <font color="DarkBlue">Argoverse Stereo Matching</font> of the CVPR2021 workshop.</li>
    <li> 2020.08: We <font color="Red">rank third</font> in the <font color="DarkBlue">BOP Challenge 2020</font> of the ECCV2020 workshop.</li>
    <li> 2021.07: One paper accepted by <font color="DarkRed">ACMMM2020</font>.</li>
    <li> 2021.01: One paper accepted by <font color="DarkRed">ICPR2020</font> as the <font color="Red">Best Scientific Paper</font>.</li>
    <li> 2020.07: Joined Baidu Inc. at Shenzhen as a computer vision researcher. Foucs on 3D Vision. </li>
    <li> 2020.06: One counting paper accepted by <font color="DarkRed">ECAI2020</font>. </li>
    <li> 2019.07: One counting paper accepted by <font color="DarkRed">BMVC2019</font>. </li>
    <li> 2019.02: One paper accepted by <font color="DarkRed">NAACL2019</font>
    <li> 2018.12: Joined Tecent AI lab at Shenzhen as research intern. Started doing research on 3D Vision. </li>
    </ul>
  </div>
</div>
</div>



<a name="pub"></a>
<div style="clear: both;">
<div class="section" style="padding-left: 0.8em; padding-top: 1em">
<h2 id="confpapers">Publications (* = co-first author)</h2>

<div class="paper" id="xxx"><img class="paper" src="papers/SPNet.png" />
  <div>
    <a><b>Paint and Distill: Boosting 3D Object Detection with Semantic Passing Network</b></a><br />
    Bo Ju*, <u><b style="color:darkred">Zhikang Zou*</b></u>, Xiaoqing Ye, Minyue Jiang, Xiao Tan, Errui Ding, Jingdong Wang<br />
    <i> ACM International Conference on Multimedia <b> <font color="DarkRed">(ACMMM)</font></b>, 2022</i> <br />
    [ <a href='https://arxiv.org/abs/2207.05497'>PDF</a> ]<br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/RIL.png" />
  <div>
    <a><b>Repainting and Imitating Learning for Lane Detection</b></a><br />
    Yue He*, Minyue Jiang*, Xiaoqing Ye*, Liang Du*, <u><b style="color:darkred">Zhikang Zou</b></u>, Wei Zhang, Xiao Tan, Errui Ding<br />
    <i> ACM International Conference on Multimedia <b> <font color="DarkRed">(ACMMM)</font></b>, 2022</i> <br />
    [ <a href='https://dl.acm.org/doi/10.1145/3503161.3548042'>PDF</a> ]<br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/SGM3D.png" />
  <div>
    <a><b>SGM3D: Stereo Guided Monocular 3D Object Detection</b></a><br />
    Zheyuan Zhou*, Liang Du*, Xiaoqing Ye*, <u><b style="color:darkred">Zhikang Zou</b></u>, Xiao Tan, Li Zhang, Xiangyang Xue, Jianfeng Feng<br />
    <i>IEEE Robotics and Automation Letters <b><font color="DarkRed">(RA-L)</font></b>, 2022 </i><br />
    [ <a href='https://arxiv.org/pdf/2112.01914.pdf'>PDF</a> ] 
    [ <a href='https://github.com/zhouzheyuan/sgm3d'>Code</a> ]<br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/DFRNet.jpeg" />
  <div>
    <a><b>The Devil is in the Task: Exploiting Reciprocal Appearance-Localization Features for Monocular 3D Object Detection</b></a><br />
    <u><b style="color:darkred">Zhikang Zou*</b></u>, Xiaoqing Ye*, Liang Du*, Xianhui Cheng*, Xiao Tan, Li Zhang, Jianfeng Feng, Xiangyang Xue, Errui Ding <br />
    <i>IEEE International Conference on Computer Vision <b><font color="DarkRed">(ICCV)</font></b>, 2021 </i><br />
    [ <a href='https://openaccess.thecvf.com/content/ICCV2021/html/Zou_The_Devil_Is_in_the_Task_Exploiting_Reciprocal_Appearance-Localization_Features_ICCV_2021_paper.html'>PDF</a> ] <br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/depth_iccv2021.png" />
  <div>
    <a><b>Revealing the Reciprocal Relations between Self-Supervised Stereo and Monocular Depth Estimation</b></a><br />
    Zhi Chen*, Xiaoqing Ye*, Wei Yang, Zhenbo Xu, Xiao Tan, <u><b style="color:darkred">Zhikang Zou</b></u>, Errui Ding, Xinming Zhang, Liusheng Huang<br />
    <i>IEEE International Conference on Computer Vision <b><font color="DarkRed">(ICCV)</font></b>, 2021 </i><br />
    [ <a href='https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Revealing_the_Reciprocal_Relations_Between_Self-Supervised_Stereo_and_Monocular_Depth_ICCV_2021_paper.html'>PDF</a> ] <br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/ASNet.png" />
  <div>
    <a><b>Coarse to Fine: Domain Adaptive Crowd Counting via Adversarial Scoring Network</b></a><br />
    <u><b style="color:darkred">Zhikang Zou*</b></u>, Xiaoye Qu*, Pan Zhou, Shuangjie Xu, Xiaoqing Ye, Wenhao Wu, Jin Ye <br />
    <i> ACM International Conference on Multimedia <b> <font color="DarkRed">(ACMMM)</font></b>, 2021</i> <br />
    [ <a href='https://arxiv.org/abs/2107.12858'>PDF</a> ] <br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/APAM.png" style="height:60px"/>
  <div>
    <a><b>Towards Adversarial Patch Analysis and Certified Defense against Crowd Counting</b></a><br />
    Qiming Wu*, <u><b style="color:darkred">Zhikang Zou*</b></u>, Pan Zhou, Xiaoqing Ye, Binghui Wang, Ang Li<br />
    <i> ACM International Conference on Multimedia <b> <font color="DarkRed">(ACMMM)</font></b>, 2021</i> <br />
    [ <a href='https://arxiv.org/abs/2104.10868'>PDF</a> ]
    [ <a href='https://github.com/harrywuhust2022/Adv-Crowd-analysis'>Code</a> ] <br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/DSANet.png" />
  <div>
    <a><b>DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning</b></a><br />
    Wenhao Wu*, Yuxiang Zhao*, Yanwu Xu, Xiao Tan, Dongliang He, <u><b style="color:darkred">Zhikang Zou</b></u>, Jin Ye, Yingying Li, Mingde Yao, Zichao Dong,
    Yifeng Shi <br />
    <i> ACM International Conference on Multimedia <b> <font color="DarkRed">(ACMMM)</font></b>, 2021</i> <br />
    [ <a href='https://arxiv.org/pdf/2105.12085.pdf'>PDF</a> ]
    [ <a href='https://github.com/whwu95/DSANet'>Code</a> ] <br />
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/CAN.png" />
  <div>
    <a><b>Attend to Where and When: Cascaded Attention Network for Facial Expression Recognition</b></a><br />
    Xiaoye Qu, <u><b style="color:darkred">Zhikang Zou</b></u>, Xinxing Su, Pan Zhou, Wei Wei, Shiping Wen, Dapeng Wu<br />
    <i> IEEE Transactions on Emerging Topics in Computational Intelligence <b> <font color="DarkRed">(TETCI)</font></b>, 2021</i> <br />
    [ <a href='https://ieeexplore.ieee.org/document/9422365/authors#authors'>PDF</a> ]
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/bop_challenge2020.jpeg" style="height:50px"/>
  <div>
    <a><b>Leaping from 2D Detection to Efficient 6DoF Object Pose Estimation</b></a><br />
    Jinhui Liu*,  <u><b style="color:darkred">Zhikang Zou*</b></u>, Xiaoqing Ye*, Xiao Tan, Errui Ding, Feng Xu, Xin Yu<br />
    <i> European Conference on Computer Vision <b> <font color="DarkRed">(ECCV)</font></b>, 2020 workshop</i> <br />
    [ <a href='https://link.springer.com/chapter/10.1007/978-3-030-66096-3_47'>PDF</a> ]<br />
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/FIAN.png" style="height:80px"/>
  <div>
    <a><b>Fine-grained Iterative Attention Network for TemporalLanguage Localization in Videos</b></a><br />
    Xiaoye Qu*, Pengwei Tang*, <u><b style="color:darkred">Zhikang Zou</b></u>, Yu Cheng, Jianfeng Dong, Pan Zhou<br />
    <i> ACM International Conference on Multimedia <b> <font color="DarkRed">(ACMMM)</font></b>, 2020</i> <br />
    [ <a href='https://dl.acm.org/doi/abs/10.1145/3394171.3414053'>PDF</a> ]<br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/HANet.png" style="height:80px"/>
  <div>
    <a><b>HANet: Hybrid Attention-aware Network for Crowd Counting</b></a><br />
    Xinxing Su, Yuchen Yuan, Xiangbo Su, <u><b style="color:darkred">Zhikang Zou</b></u>, Shilei Wen, Pan Zhou<br />
    <i> International Conference on Pattern Recognition<b> <font color="DarkRed">(ICPR)</font></b>, 2020</i> <br />
    <font color="Red", style="font-weight:bold">Best Scientific Paper</font> [ <a href='https://ieeexplore.ieee.org/document/9412883/figures#figures'>PDF</a> ]<br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/HSRNet.png"/>
  <div>
    <a><b>Crowd Counting via Hierarchical Scale Recalibration Network</b></a><br />
    <u><b style="color:darkred">Zhikang Zou*</b></u>, Yifan Liu*, Shuangjie Xu, Wei Wei, Shiping Wen, Pan Zhou<br />
    <i> European Conference on Artificial Intelligence <b> <font color="DarkRed">(EACI)</font></b>, 2020</i> <br />
    [ <a href='https://arxiv.org/abs/2003.03545'>PDF</a> ] <br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/E3D.jpeg"/>
  <div>
    <a><b>Enhanced 3D Convolutional Networks for Crowd Counting</b></a><br />
    <u><b style="color:darkred">Zhikang Zou*</b></u>, Huiliang Shao*, Xiaoye Qu, Wei Wei, Pan Zhou<br />
    <i> British Machine Vision Conference <b> <font color="DarkRed">(BMVC)</font></b>, 2019</i> <br />
    [ <a href='https://arxiv.org/abs/1908.04121'>PDF</a> ]<br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/ACAN.png"/>
  <div>
    <a><b>Adversarial Category Alignment Network for Cross-domain Sentiment Classification</b></a><br />
    Xiaoye Qu*, <u><b style="color:darkred">Zhikang Zou*</b></u>, Yu Cheng, Yang Yang, Pan Zhou<br />
    <i> Annual Conference of the North American Chapter of the Association for Computational Linguistics <b> <font color="DarkRed">(NAACL)</font></b>, 2019</i> <br />
    [ <a href='https://aclanthology.org/N19-1258/'>PDF</a> ]
    [ <a href='https://github.com/XiaoYee/ACAN'>Code</a> ] <br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/ACM-CNN.png" style="height:100px"/>
  <div>
    <a><b>Attend To Count: Crowd Counting with Adaptive Capacity Multi-scale CNNs</b></a><br />
    <u><b style="color:darkred">Zhikang Zou</b></u>, Yu Cheng, Xiaoye Qu, Shouling Ji, Xiaoxiao Guo, Pan Zhou<br />
    <i> <b> <font color="DarkRed">Neurocomputing</font></b>, 2019</i> <br />
    [ <a href='https://arxiv.org/abs/1908.02797'>PDF</a> ]<br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>

<div class="paper" id="xxx"><img class="paper" src="papers/DANet.png" style="height:100px"/>
  <div>
    <a><b>DA-Net: Learning the fine-grained density distribution with deformation aggregation network</b></a><br />
    <u><b style="color:darkred">Zhikang Zou</b></u>, Xinxing Su, Xiaoye Qu, Pan Zhou<br />
    <i> <b> <font color="DarkRed">IEEE Access</font></b>, 2018</i> <br />
    [ <a href='https://ieeexplore.ieee.org/document/8497050'>PDF</a> ]
    [ <a href='https://github.com/BigTeacher-777/DA-Net-Crowd-Counting'>Code</a> ]<br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section" style="padding-left: 0.8em; padding-top: 1em">
<h2 id="confpapers">Contests</h2>
<div class="paper">
<ul>
<li>CVPR2021 Argoverse Stereo Matching: 2nd in contest setting.</li>
<li>ECCV2020 BOP Challenge: 3st place in RGB-based pose estimation</li>
</ul>
<div class="spanner"></div>
</div>
</div>
</div>

<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 24st July, 2021</a></font></p>
</div>


</body>
</html>
